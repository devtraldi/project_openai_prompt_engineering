{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c615a5e6-315f-4fe4-b2c9-254d3fef704e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Environment Variables for Google ---\n",
      "Google API Key loaded successfully.\n",
      "Google AI client is configured and ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup - Imports and Google Gemini API Configuration\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"--- Step 1: Loading Environment Variables for Google ---\")\n",
    "\n",
    "# Load variables from the .env file.\n",
    "# Make sure your .env file has the GOOGLE_API_KEY.\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Safety check to ensure the key was found.\n",
    "if not google_api_key:\n",
    "    raise ValueError(\"Google API key not found. Check your .env file for the GOOGLE_API_KEY variable.\")\n",
    "else:\n",
    "    print(\"Google API Key loaded successfully.\")\n",
    "\n",
    "# Configure the Google AI library with your key.\n",
    "genai.configure(api_key=google_api_key)\n",
    "print(\"Google AI client is configured and ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d6c099f-d8c7-4c34-bb67-4625f2291142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Available Gemini Models ---\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-05-20\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-flash-lite-preview-06-17\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.5-pro-preview-06-05\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-2.5-flash-lite\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: List Models\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Make sure you have already run the cell that configures your API key\n",
    "print(\"--- Available Gemini Models ---\")\n",
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b9f1db-69e8-42ba-a363-4724aabdfc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Model Initialization and Prompt Execution\n",
    "print(\"\\n--- Step 2: Initializing the Gemini Pro model ---\")\n",
    "\n",
    "try:\n",
    "    # We select the model we want to use. 'gemini-pro' is excellent for text tasks.\n",
    "    model = genai.GenerativeModel('gemini-2.5-flash-lite')\n",
    "    print(\"Model 'gemini-2.5-flash-lite' initialized successfully.\")\n",
    "\n",
    "    # --- Sending a well-structured prompt ---\n",
    "    print(\"\\n--- Step 3: Sending prompt to Gemini ---\")\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    Explain the concept of 'prompt engineering' to a junior developer.\n",
    "    Structure your answer with:\n",
    "    1. A one-sentence concept.\n",
    "    2. A simple, real-world analogy.\n",
    "    3. The main goal of using it.\n",
    "    4. Final generic thoughts\n",
    "    \"\"\"\n",
    "\n",
    "    # Send the prompt to the model to generate content\n",
    "    response = model.generate_content(prompt)\n",
    "\n",
    "    # The response text is accessed directly via 'response.text'\n",
    "    print(\"\\n>>> Response from Gemini:\")\n",
    "    print(response.text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e69ea3-bc3f-4d6f-b3cc-fe9083065ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Advanced Prompting Example: Book Recommender ---\n",
      "Model initialized with a custom 'Book Recommender' personality.\n",
      "Chat session started with examples.\n",
      "\n",
      "--- Sending new user prompt to the configured model ---\n",
      "\n",
      ">>> Personalized Recommendation from Gemini:\n",
      "Given your enjoyment of the noirish, mystery elements in *Blade Runner*, I recommend **\"Blindsight\" by Peter Watts**.  It's a fast-paced sci-fi thriller with a strong mystery at its core, exploring themes of identity and perception in a way that echoes the philosophical questions raised in *Blade Runner*.  The pacing is relentless and the mystery keeps you guessing until the very end.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 4: ADVANCED PROMPTING WITH SYSTEM INSTRUCTIONS AND EXAMPLES\n",
    "# ==============================================================================\n",
    "import google.generativeai as genai\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- Advanced Prompting Example: Book Recommender ---\")\n",
    "\n",
    "# --- 1. Define the System Prompt ---\n",
    "# This is the main directive for the AI's personality and task.\n",
    "system_instruction = \"\"\"\n",
    "You are a friendly and knowledgeable AI assistant that generates personalized book recommendations.\n",
    "Your goal is to suggest one book based on the user's preferences.\n",
    "Always state the reason for your recommendation, linking it back to the user's input.\n",
    "\"\"\"\n",
    "\n",
    "# --- 2. Initialize the Model with the System Prompt ---\n",
    "# We pass the instruction directly when creating the model.\n",
    "# Note: Newer models like 'gemini-1.5-flash' are excellent at following system instructions.\n",
    "try:\n",
    "    model = genai.GenerativeModel(\n",
    "        'gemini-1.5-flash',\n",
    "        system_instruction=system_instruction\n",
    "    )\n",
    "    print(\"Model initialized with a custom 'Book Recommender' personality.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not initialize model: {e}\")\n",
    "\n",
    "\n",
    "# --- 3. Create a Conversation History with Examples (Few-shot Prompting) ---\n",
    "# This shows the model how to behave.\n",
    "# Note that OpenAI's 'assistant' role is called 'model' in the Gemini API.\n",
    "chat_history = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\"I love epic fantasy with complex magic systems, like 'The Lord of the Rings'.\"]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [\"Based on your love for epic fantasy, I recommend **'The Name of the Wind' by Patrick Rothfuss**. It features a beautifully intricate magic system and deep world-building that you might enjoy.\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# --- 4. Start a \"Chat Session\" with the history ---\n",
    "# This is the best way to handle multi-turn conversations.\n",
    "chat = model.start_chat(history=chat_history)\n",
    "print(\"Chat session started with examples.\")\n",
    "\n",
    "\n",
    "# --- 5. Send the New User Prompt ---\n",
    "print(\"\\n--- Sending new user prompt to the configured model ---\")\n",
    "\n",
    "new_prompt = \"I'm looking for a fast-paced sci-fi mystery. I really liked 'Blade Runner'.\"\n",
    "\n",
    "try:\n",
    "    # We send the new message within the context of the chat session\n",
    "    response = chat.send_message(new_prompt)\n",
    "\n",
    "    print(\"\\n>>> Personalized Recommendation from Gemini:\")\n",
    "    print(response.text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2886d04-8d7b-40b3-8d65-071a2a8819a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
